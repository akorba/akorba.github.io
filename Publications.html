<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!--<meta name="viewport" content="width=device-width, shrink-to-fit=no, initial-scale=1">-->
    <meta name="viewport" content="user-scalable = yes">

    <meta name="description" content="">
    <meta name="author" content="">

    <title>Anna Korba</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/simple-sidebar.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <div id="wrapper", class="toggled">

        <!-- Sidebar -->
        <div id="sidebar-wrapper">
            <ul class="sidebar-nav">
                <li class="sidebar-brand">
                    <a href="#">
                        <a href="index.html">Home</a>
                    </a>
                </li>
                <li>
                    <a href="#"> <a href="Publications.html">Papers</a> </a>
                </li>
                <li>
                    <a href="#"> <a href="Talks.html">Talks</a></a>
                </li>
                <li>
                    <a href="#"><a href="Teaching.html">Teaching</a></a>
                </li>
                                <li>
                    <a href="#"><a href="MasterDS.html">ENSAE/M.DS</a></a>
                </li>
                <li>
                    <a href="#"><a href="Contact.html">Contact</a></a>
                </li>
                <<!--<li>
                    <a href="#">Services</a>
                </li>
                <li>
                    <a href="#">Contact</a>
                </li>-->
            </ul>
        </div>
        <!-- /#sidebar-wrapper -->

        <!-- Page Content -->
        <div id="page-content-wrapper">
            <div class="container-fluid">
                <div class="row">
                    <div class="col-lg-12">
                        <h1 style="margin-bottom: 50px;">Papers</h1>
                       <!-- <h3 style="border-bottom: 1px dashed grey;">Preprints</h3>-->

    

                      <p style="font-size: 18px;">Most of my publications can be found on my <a href="https://scholar.google.com/citations?user=dbH6E3kAAAAJ&hl=fr&oi=ao">Google scholar profile</a>, below they are classified by themes.</p>


                        <h3 style="border-bottom: 1px dashed grey;">Sampling/Optimization over measures</h3>      

                         <ul style="padding-left:0;margin-left: 1.2em;font-size: 18px;">
                         	 <li> A Vacher, O. Chehab, A. Korba. <b> Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics.</b>
                             <i>Submitted, 2024.</i>    
                             <br>
                             [<a href="https://arxiv.org/abs/2501.00565">paper</a>] 
                             <li> O. Chehab, A. Korba, A. Stromme, A Vacher. <b> Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics.</b>
                             <i>International Conference on Learning Representations (ICLR), 2025.</i>    
                             <br>
                            [<a href="https://arxiv.org/abs/2410.09697">paper</a>] 
                            <li> Z. Chen, A. Mustafi, P. Glaser, A. Korba, A. Gretton, B. Sriperumbudur. <b> (De)-regularized Maximum Mean Discrepancy Gradient Flow.</b>
                             <i>Submitted, 2024.</i>    
                             <br>
                            [<a href="https://arxiv.org/abs/2409.14980">paper</a>] 
                            <li> L.F.O. Chamon, M.R. Karimi, A. Korba. <b> Constrained Sampling with Primal-Dual Langevin Monte Carlo.</b>
                             <i>Advances in Neural Information Processing Systems (NeurIPS), 2024.</i>    
                             <br>
                            [<a href="https://arxiv.org/abs/2411.00568">paper</a>] 
                         	  <li> C. Chazal, A. Korba, F. Bach. <b> Statistical and Geometrical properties of regularized Kernel Kullback-Leibler divergence.</b>
                             <i>Advances in Neural Information Processing Systems (NeurIPS), 2024.</i>    
                             <br>
                            [<a href="https://arxiv.org/abs/2408.16543">paper</a>] 
                            <li> C. Bonet, T. Uscidda, A. David, P-C. Aubin-Frankowski, A. Korba. <b> Mirror and Preconditioned Gradient Descent in
                            Wasserstein Space.</b>
                             <i>Advances in Neural Information Processing Systems (NeurIPS), 2024.</i>    
                             <br>
                            [<a href="https://arxiv.org/abs/2406.08938">paper</a>] 

                             <li> P. Marion, A. Korba, P. Bartlett, M. Blondel, V. De Bortoli, A. Doucet, F. Llinares-López, C. Paquette, Quentin Berthet. <b> Implicit Diffusion: Efficient Optimization through Stochastic Sampling.</b>
                             <i>Submitted, 2024.</i> 
                                 <br>
                                [<a href="https://arxiv.org/abs/2402.05468">paper</a>] 
                              <li> T. Huix, A. Korba, A. Durmus, E. Moulines.<b> Theoretical Guarantees for Variational Inference with Fixed-Variance Mixture of Gaussians.</b>
                             <i>International Conference of Machine Learning (ICML), 2024.</i> 
                                 <br>
                                [<a href="">paper</a>] 
                             <li> N. Chopin, F. Crucinio , A. Korba. <b> A connection between Tempering and Entropic Mirror Descent.</b>
                             <i>International Conference of Machine Learning (ICML), 2024.</i> 
                                 <br>
                                [<a href="https://arxiv.org/pdf/2310.11914.pdf">paper</a>] 
                             <li> 
                                 L. Li, Q. Liu, A. Korba, M. Yurochkin, J. Solomon.<b> Sampling with Mollified Interaction Energy Descent.</b> <i>International Conference on Learning Representations (ICLR), 2023.</i> 
                                 <br>
                                [<a href="https://arxiv.org/pdf/2210.13400.pdf">paper</a>] 
                            <li> 
                                 L. Xu, A. Korba, D. Slepcev.<b> Accurate Quantization of Measures via Interacting Particle-based Optimization.</b> <i>International Conference of Machine Learning (ICML), 2022.</i> 
                                 <br>
                                [<a href="https://proceedings.mlr.press/v162/xu22d.html">paper</a>] 
                           <li> 
                                 T. Huix, S. Majewski, A. Durmus, E. Moulines, A. Korba.<b> Variational Inference of overparameterized Bayesian Neural Networks: a theoretical and empirical study.</b> <i>Submitted, 2022.</i> 
                                 <br>
                                [<a href="https://arxiv.org/abs/2207.03859">paper</a>] 
                            <li> 
                                 P-C. Aubin-Frankowski, A. Korba, F. Léger.<b> Mirror Descent with Relative Smoothness in Measure Spaces, with application to Sinkhorn and EM.</b> <i>Advances in Neural Information Processing Systems (NeurIPS), 2022.</i> 
                                 <br>
                                [<a href="https://arxiv.org/abs/2206.08873">paper</a>] 
                            <li> 
                                 A. Korba, F. Portier.<b> Adaptive Importance Sampling meets Mirror Descent: a Bias-variance tradeoff.</b> In <i>Artificial Intelligence and Statistics (AISTATS), 2022.</i> Accepted for Oral presentation (top 10%).
                                 <br>
                                [<a href="https://arxiv.org/abs/2110.15590">paper</a>][<a href="resources/aistats2022poster.pdf">poster</a>][<a href="resources/Aistats_March2022.pdf">slides</a>][<a href="https://virtual.aistats.org/virtual/2022/oral/3647">video</a>]
                         	<li> 
                                 A. Korba, P.-C. Aubin-Frankowski, S. Majewski, P. Ablin.<b> Kernel Stein Discrepancy Descent.</b> In<i> International Conference of Machine Learning (ICML) 2021.</i>  Accepted for Long Oral presentation (top 15%). <br>
                                [<a href="https://arxiv.org/abs/2105.09994">paper</a>][<a href="resources/poster_KSD_Descent.pdf">poster</a>][<a href="resources/ICMLtalk_July2021.pdf">slides</a>][<a href="https://icml.cc/virtual/2021/oral/9274">video</a>]
                                  </li>
                            <li> 
                                 A. Korba, A. Salim, M. Arbel, G. Luise, A. Gretton. <b>A Non-Asymptotic Analysis for Stein Variational Gradient Descent.</b>   In<i> Advances in Neural Information Processing Systems (NeurIPS) 2020.</i> <br>
                                [<a href="https://arxiv.org/abs/2006.09797">paper</a>][<a href="resources/poster_SVGD.pdf">poster</a>]
                                 </li>
                            <li> 
                                 A. Salim, A. Korba, G. Luise. <b>Wasserstein Proximal Gradient.</b> In <i>Advances in Neural Information Processing Systems (NeurIPS) 2020.</i> <br>
                                [<a href="https://arxiv.org/abs/2002.03035">paper</a>][<a href="resources/poster_Wass_prox.pdf">poster</a>]
                                 </li>
                            <li> 
                                 M. Arbel, A. Korba, A. Salim , A.Gretton. <b>Maximum Mean Discrepancy Gradient flow.</b> In<i> Advances in Neural Information Processing Systems (NeurIPS) 2019.</i> <br>
                                [<a href="https://arxiv.org/abs/1906.04370">paper</a>][<a href="https://github.com/MichaelArbel/MMD-gradient-flow">code</a>]
                                 </li>
                        </ul>
                        <h3 style="border-bottom: 1px dashed grey;">Off-Policy Learning and Evaluation</h3>
                        <ul style="padding-left:0;margin-left: 1.2em;font-size: 18px;">

                            <li> 
                                 I. Aouali, V-E. Brunel, D. Rohde, A. Korba. <b>Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
.</b>   <i> Submitted, 2024.</i> <br>
                                [<a href="https://arxiv.org/abs/2402.14664">paper</a>]
                                  </li>
                                    <li>  I. Aouali, V-E. Brunel, D. Rohde, A. Korba. <b>Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling.</b>   <i> Uncertainty in Artificial Intelligence, 2024.</i> <br>
                                [<a href="https://arxiv.org/abs/2406.03434">paper</a>]
                                  </li>
                                 <li> 
                                 I. Aouali, V-E. Brunel, D. Rohde, A. Korba. <b>Exponential Smoothing for Off-Policy Learning.</b>   In<i> International Conference of Machine Learning (ICML) 2023.</i> <br>
                                [<a href="https://arxiv.org/abs/2305.15877">paper</a>][<a href="https://openreview.net/forum?id=LJ9iKElXpl" >supplementary material</a>]
                                  </li>
                         </ul>

                        <h3 style="border-bottom: 1px dashed grey;">Causality</h3>
                        <ul style="padding-left:0;margin-left: 1.2em;font-size: 18px;">
                                 <li> 
                                 A. Mastouri, Y. Zhu, L. Gultchin, A. Korba, R. Silva, M. J. Kusner, A. Gretton, K. Muandet. <b>Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction.</b>   In<i> International Conference of Machine Learning (ICML) 2021.</i> <br>
                                [<a href="https://arxiv.org/abs/2105.04544">paper</a>]
                                  </li>
                         </ul>

                        <h3 style="border-bottom: 1px dashed grey;">Ranking</h3>
                         <p style="font-size: 18px;">During my PhD, I studied how to analyze preference data in the form of total orders/<b>permutations</b> or <b>pairwise comparisons</b>, and  how to handle statistical problems related to such data, including ranking aggregation, distribution estimation, and prediction.
                        Here is the final version of the <a href="resources/manuscript.pdf">manuscript</a> and the <a href="resources/phdslides2018.pdf">slides</a>; and below a list of publications.</p>
                        <ul style="padding-left:0;margin-left: 1.2em;font-size: 18px;">
                            <li> M. Achab (*), A. Korba (*), S. Clémençon.<b> Dimensionality Reduction for (Bucket) Ranking: A Mass Transportation Approach.</b> In <i>Algorithmic Learning Theory (ALT) 2019.</i>  (* :) equal contribution
                                <br>  
                                [<a href="https://arxiv.org/pdf/1810.06291.pdf">paper</a>][<a href="https://github.com/akorba/Dimensionality_Reduction_Ranking">code</a>]
                                
                                  </li></li>
                            <li> A. Korba, A. Garcia, F. D'Alché-Buc. <b>A Structured Prediction Approach for Label Ranking. </b>In<br>
                                <i>Advances in Neural Information Processing Systems (NeurIPS) 2018.</i><br>
                                  [<a href="https://papers.nips.cc/paper/8114-a-structured-prediction-approach-for-label-ranking">paper</a>][<a href="https://github.com/akorba/Structured_Approach_Label_Ranking">code</a>][<a href="resources/poster_structured_label_ranking.pdf">poster</a>]
                                  </li></li>
                                  
                            <li> S. Clémençon, A. Korba.<b> On Aggregation in Ranking Median Regression. </b>In <i>European Symposium on Artificial Neural Networks (ESANN) 2018.</i> <br>
                                 [<a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2018-124.pdf">paper</a>]
                                  </li></li>
                                  
                              <li> S. Clémençon, A. Korba, E. Sibony. <b>Ranking Median Regression: Learning to Order through Local Consensus. </b>
                                 In <i>Algorithmic Learning Theory (ALT) 2018.</i><br>
                                [<a href="https://arxiv.org/pdf/1711.00070.pdf">Arxiv long version </a>] [<a href= http://www.cs.cornell.edu/conferences/alt2018/A/Clemencon18Paper.pdf >ALT short version</a>] <!--[<a href="resources/RMR_WS_submission.pdf" >NIPS 2017 workshop version</a>]  -->
                            </li></li>
                            <li> A. Korba, S. Clémençon, E. Sibony. <b>A Learning Theory of Ranking Aggregation.</b> In <i>Artificial Intelligence and Statistics (AISTATS) 2017.</i><br>
                                [<a href="http://proceedings.mlr.press/v54/korba17a.html">paper</a>]  [<a href="resources/aistats2017poster.pdf">poster</a>] </li></li>
                            <li> Y. Jiao, A. Korba, E. Sibony. <b>Controlling the distance to a Kemeny consensus without computing it.</b>  In
                                <i> International Conference on Machine Learning (ICML) 2016.</i><br>
                            [<a href="http://proceedings.mlr.press/v48/korba16.pdf">paper</a>]  [<a href="resources/icml2016poster.pdf">poster</a>] </li>
                        </ul>

        <!-- /#page-content-wrapper -->

    </div>
    <!-- /#wrapper -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Menu Toggle Script -->
    <script>
    $("#menu-toggle").click(function(e) {
        e.preventDefault();
        $("#wrapper").toggleClass("toggled");
    });
    </script>

</body>

</html>
